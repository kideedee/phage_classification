{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-30T09:46:30.112586Z",
     "start_time": "2025-04-30T09:46:30.104587Z"
    }
   },
   "source": [
    "import gc\n",
    "import os\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.amp import GradScaler, autocast\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from common.env_config import config"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T09:46:30.236582Z",
     "start_time": "2025-04-30T09:46:30.206547Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def classification_report_csv(report, path_save, c):\n",
    "    report_data = []\n",
    "    lines = report.split('\\n')\n",
    "    for line in lines[2:-3]:\n",
    "        # Skip empty lines\n",
    "        if not line.strip():\n",
    "            continue\n",
    "\n",
    "        row = {}\n",
    "        row_data = line.split()\n",
    "\n",
    "        # Skip lines that don't have enough data\n",
    "        if len(row_data) < 5:\n",
    "            continue\n",
    "\n",
    "        row['class'] = row_data[0]\n",
    "        row['precision'] = float(row_data[1])\n",
    "        row['recall'] = float(row_data[2])\n",
    "        row['f1_score'] = float(row_data[3])\n",
    "        row['support'] = float(row_data[4])\n",
    "        report_data.append(row)\n",
    "    if c == 0:\n",
    "        dataframe = pd.DataFrame.from_dict(report_data)\n",
    "        dataframe.to_csv(path_save + 'classification_report.csv', index=False)\n",
    "    else:\n",
    "        print('train')\n",
    "    return report_data\n",
    "\n",
    "\n",
    "# Calculate sensitivity (recall) and specificity\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # For binary classification, confusion matrix is [[TN, FP], [FN, TP]]\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    # Calculate metrics\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    f1 = 2 * precision * sensitivity / (precision + sensitivity) if (precision + sensitivity) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        'sensitivity': sensitivity,\n",
    "        'specificity': specificity,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'f1_score': f1\n",
    "    }\n",
    "\n",
    "\n",
    "# Define the model\n",
    "class DeePhage(nn.Module):\n",
    "    def __init__(self, max_length):\n",
    "        super(DeePhage, self).__init__()\n",
    "\n",
    "        # Conv1D layer with 64 filters, kernel size 6, ReLU activation\n",
    "        self.conv = nn.Conv1d(4, 64, kernel_size=6, padding='same')\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # MaxPooling1D with pool size 3\n",
    "        self.pool = nn.MaxPool1d(3)\n",
    "\n",
    "        # BatchNormalization\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "\n",
    "        # Dropout (0.3)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        # Fully connected layers\n",
    "        # Calculate feature size after pooling\n",
    "        self.feature_size = max_length // 3  # After pooling with size 3\n",
    "\n",
    "        self.fc1 = nn.Linear(64, 64)  # Using GlobalAveragePooling instead of flattening\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # PyTorch Conv1d expects [batch, channels, length] format\n",
    "        # Input is [batch, length, channels], so transpose dimensions\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        # Apply convolution and activation\n",
    "        x = self.relu(self.conv(x))\n",
    "\n",
    "        # Apply pooling\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Apply batch normalization and dropout\n",
    "        x = self.bn1(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Global average pooling (equivalent to GlobalAveragePooling1D in Keras)\n",
    "        x = torch.mean(x, dim=2)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.bn2(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# History tracking class (similar to Keras' LossHistory callback)\n",
    "class LossHistory:\n",
    "    def __init__(self):\n",
    "        self.losses = {'batch': [], 'epoch': []}\n",
    "        self.accuracy = {'batch': [], 'epoch': []}\n",
    "        self.val_loss = {'batch': [], 'epoch': []}\n",
    "        self.val_acc = {'batch': [], 'epoch': []}\n",
    "        self.sensitivity = {'epoch': []}\n",
    "        self.specificity = {'epoch': []}\n",
    "        self.val_sensitivity = {'epoch': []}\n",
    "        self.val_specificity = {'epoch': []}\n",
    "\n",
    "    def update_batch(self, loss, acc, val_loss=None, val_acc=None):\n",
    "        self.losses['batch'].append(loss)\n",
    "        self.accuracy['batch'].append(acc)\n",
    "        if val_loss is not None:\n",
    "            self.val_loss['batch'].append(val_loss)\n",
    "        if val_acc is not None:\n",
    "            self.val_acc['batch'].append(val_acc)\n",
    "\n",
    "    def update_epoch(self, loss, acc, val_loss, val_acc, sensitivity=None, specificity=None,\n",
    "                     val_sensitivity=None, val_specificity=None):\n",
    "        self.losses['epoch'].append(loss)\n",
    "        self.accuracy['epoch'].append(acc)\n",
    "        self.val_loss['epoch'].append(val_loss)\n",
    "        self.val_acc['epoch'].append(val_acc)\n",
    "\n",
    "        if sensitivity is not None:\n",
    "            self.sensitivity['epoch'].append(sensitivity)\n",
    "        if specificity is not None:\n",
    "            self.specificity['epoch'].append(specificity)\n",
    "        if val_sensitivity is not None:\n",
    "            self.val_sensitivity['epoch'].append(val_sensitivity)\n",
    "        if val_specificity is not None:\n",
    "            self.val_specificity['epoch'].append(val_specificity)\n",
    "\n",
    "    def loss_plot(self, loss_type, accuracy, viru_acc, temp_acc, train_viru_acc, train_temp_acc,\n",
    "                  path_save, max_length, lr_rate, b_size):\n",
    "        iters = range(len(self.losses[loss_type]))\n",
    "\n",
    "        # Original loss/accuracy plot\n",
    "        plt.figure(figsize=(16, 8))\n",
    "\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(iters, self.accuracy[loss_type], 'r', label='train acc')\n",
    "        plt.plot(iters, self.losses[loss_type], 'g', label='train loss')\n",
    "        if loss_type == 'epoch':\n",
    "            plt.plot(iters, self.val_acc[loss_type], 'b', label='val acc')\n",
    "            plt.plot(iters, self.val_loss[loss_type], 'k', label='val loss')\n",
    "        plt.grid(True)\n",
    "        plt.ylim((0, 2))\n",
    "        plt.xlabel(loss_type)\n",
    "        plt.ylabel('acc-loss')\n",
    "        plt.title('%s test_acc: %s \\n test--- viru: %s temp: %s \\ntrain---viru: %s temp: %s'\n",
    "                  % (str(max_length), accuracy, viru_acc, temp_acc,\n",
    "                     train_viru_acc, train_temp_acc))\n",
    "        plt.legend(loc=\"upper right\")\n",
    "\n",
    "        # Additional sensitivity/specificity plot\n",
    "        if loss_type == 'epoch' and hasattr(self, 'sensitivity') and len(self.sensitivity['epoch']) > 0:\n",
    "            plt.subplot(2, 1, 2)\n",
    "            plt.plot(iters, self.sensitivity[loss_type], 'r', label='train sensitivity')\n",
    "            plt.plot(iters, self.specificity[loss_type], 'g', label='train specificity')\n",
    "            plt.plot(iters, self.val_sensitivity[loss_type], 'b', label='val sensitivity')\n",
    "            plt.plot(iters, self.val_specificity[loss_type], 'k', label='val specificity')\n",
    "            plt.grid(True)\n",
    "            plt.ylim((0, 1.1))\n",
    "            plt.xlabel(loss_type)\n",
    "            plt.ylabel('sensitivity-specificity')\n",
    "            plt.legend(loc=\"lower right\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(path_save + str(max_length) + '_' + str(lr_rate) + '_' + str(b_size) + '.png')\n",
    "        plt.close()\n",
    "\n",
    "        # Create additional plot just for sensitivity/specificity\n",
    "        if loss_type == 'epoch' and hasattr(self, 'sensitivity') and len(self.sensitivity['epoch']) > 0:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(iters, self.sensitivity[loss_type], 'r', label='train sensitivity')\n",
    "            plt.plot(iters, self.specificity[loss_type], 'g', label='train specificity')\n",
    "            plt.plot(iters, self.val_sensitivity[loss_type], 'b', label='val sensitivity')\n",
    "            plt.plot(iters, self.val_specificity[loss_type], 'k', label='val specificity')\n",
    "            plt.grid(True)\n",
    "            plt.ylim((0, 1.1))\n",
    "            plt.xlabel(loss_type)\n",
    "            plt.ylabel('sensitivity-specificity')\n",
    "            plt.title('Sensitivity and Specificity Metrics')\n",
    "            plt.legend(loc=\"lower right\")\n",
    "            plt.savefig(path_save + str(max_length) + '_' + str(lr_rate) + '_' +\n",
    "                        str(b_size) + '_sensitivity_specificity.png')\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "# Binary accuracy calculation\n",
    "def binary_accuracy(y_pred, y_true):\n",
    "    y_pred_sigmoid = torch.sigmoid(y_pred)  # Apply sigmoid here\n",
    "    y_pred_tag = (y_pred_sigmoid > 0.5).float()\n",
    "    correct_results_sum = (y_pred_tag == y_true).sum().float()\n",
    "    acc = correct_results_sum / y_true.shape[0]\n",
    "    return acc.item()"
   ],
   "id": "2df3f5a9b283402b",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T09:46:30.379316Z",
     "start_time": "2025-04-30T09:46:30.242592Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Check GPU availability (since you have RTX 5070Ti)\n",
    "if torch.cuda.is_available():\n",
    "    # Monitor initial GPU memory\n",
    "    initial_mem = torch.cuda.memory_allocated(0) / (1024 ** 3)\n",
    "    print(f\"Initial GPU memory usage: {initial_mem:.2f} GB\")\n",
    "    device_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)\n",
    "    print(f\"Training on GPU: {device_name} with {gpu_memory:.2f} GB memory\")\n",
    "\n",
    "    # Clear memory and cache before training\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    torch.set_float32_matmul_precision('highest')\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    # Log CUDA information\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(\n",
    "        f\"cuDNN Version: {torch.backends.cudnn.version() if torch.backends.cudnn.is_available() else 'Not available'}\")\n",
    "\n",
    "    # Blackwell optimized kernel tuning\n",
    "    try:\n",
    "        # Set environment variables for Blackwell\n",
    "        os.environ['CUDA_AUTO_TUNE'] = '1'\n",
    "        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "        os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "        print(\"Set optimized kernel autotuning for Blackwell\")\n",
    "    except:\n",
    "        pass\n",
    "else:\n",
    "    print(\"No GPU available, training on CPU\")"
   ],
   "id": "5aa526693f1034c5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial GPU memory usage: 0.02 GB\n",
      "Training on GPU: NVIDIA GeForce RTX 5070 Ti with 15.92 GB memory\n",
      "CUDA Version: 12.8\n",
      "cuDNN Version: 90701\n",
      "Set optimized kernel autotuning for Blackwell\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T09:47:14.453170Z",
     "start_time": "2025-04-30T09:46:30.444717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "min_length = 400\n",
    "max_length = 800\n",
    "group = f\"{min_length}_{max_length}\"\n",
    "lr_rate = 0.0001\n",
    "b_size = 32\n",
    "num_epochs = 5\n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "data_dir = os.path.join(config.MY_DATA_DIR, f\"one_hot/{group}\")\n",
    "\n",
    "num_fold = 1\n",
    "fold = 1\n",
    "print(f\"\\n{'=' * 50}\")\n",
    "print(f\"Starting fold {fold}/5\")\n",
    "print(f\"{'=' * 50}\")\n",
    "\n",
    "# Path definitions\n",
    "path_save = os.path.join(config.RESULT_DIR, f\"dee_phage/{group}/fold_{fold}/{timestamp}\")\n",
    "os.makedirs(path_save, exist_ok=True)  # Ensure directory exists\n",
    "\n",
    "predict_save_path = path_save + str(max_length) + '_' + str(lr_rate) + '_' + str(b_size) + '_prediction.csv'\n",
    "model_save_path = path_save + str(max_length) + '_' + str(lr_rate) + '_' + str(b_size) + '_model.pt'\n",
    "\n",
    "# Load data from .mat files\n",
    "print('Loading data...')\n",
    "train_matrix = np.load(os.path.join(data_dir, f'one_hot_{group}_train_vector.npy'))\n",
    "train_label = np.load(os.path.join(data_dir, f'y_train.npy'))\n",
    "test_matrix = np.load(os.path.join(data_dir, f'one_hot_{group}_val_vector.npy'))\n",
    "test_label = np.load(os.path.join(data_dir, f'y_val.npy'))\n",
    "\n",
    "print(f\"Training data distribution\")\n",
    "label_counts = Counter(train_label)\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"Label {label}: {count} samples\")\n",
    "\n",
    "train_num = train_label.shape[0]\n",
    "test_num = test_label.shape[0]\n",
    "\n",
    "print(f\"Train samples: {train_num}, Test samples: {test_num}\")\n",
    "\n",
    "train_matrix = train_matrix.reshape(train_num, max_length, 4)\n",
    "test_matrix = test_matrix.reshape(test_num, max_length, 4)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "train_matrix_tensor = torch.FloatTensor(train_matrix)\n",
    "train_label_tensor = torch.FloatTensor(train_label)\n",
    "test_matrix_tensor = torch.FloatTensor(test_matrix)\n",
    "test_label_tensor = torch.FloatTensor(test_label)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = TensorDataset(train_matrix_tensor, train_label_tensor)\n",
    "test_dataset = TensorDataset(test_matrix_tensor, test_label_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=b_size, num_workers=4, prefetch_factor=4, shuffle=True,\n",
    "                          persistent_workers=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=b_size, num_workers=4, prefetch_factor=4, shuffle=False,\n",
    "                         persistent_workers=True)\n",
    "\n",
    "# Initialize model\n",
    "model = DeePhage(max_length)\n",
    "model.to(device)\n",
    "\n",
    "# Print model summary\n",
    "print(f\"Model architecture: {model}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr_rate)\n",
    "scaler = GradScaler('cuda')\n",
    "\n",
    "# Initialize history logger\n",
    "history = LossHistory()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    train_preds = []\n",
    "    train_labels = []\n",
    "\n",
    "    # Print epoch info\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} [Train] - Processing\")\n",
    "\n",
    "    # Iterate through training data\n",
    "    total_batches = len(train_loader)\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        # inputs, labels = inputs.to(device), labels.to(device)\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.view(-1, 1).to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        with autocast('cuda'):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        acc = binary_accuracy(outputs, labels)\n",
    "\n",
    "        # Collect predictions and labels for metrics\n",
    "        preds = (torch.sigmoid(outputs) > 0.5).float().cpu().detach().numpy()\n",
    "        train_preds.extend(preds)\n",
    "        train_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Update batch statistics\n",
    "        running_loss += loss.detach().item()\n",
    "        running_acc += acc\n",
    "        history.update_batch(running_loss, acc)\n",
    "\n",
    "        # Print progress every 10% of batches\n",
    "        if batch_idx % max(1, total_batches // 10) == 0 or batch_idx == total_batches - 1:\n",
    "            print(f\"  Batch {batch_idx + 1}/{total_batches} - Loss: {loss.detach().item():.4f}, Acc: {acc:.4f}\")\n",
    "\n",
    "    # Calculate epoch-level training metrics\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_acc = running_acc / len(train_loader)\n",
    "\n",
    "    # Calculate sensitivity and specificity for training data\n",
    "    train_metrics = calculate_metrics(np.array(train_labels), np.array(train_preds))\n",
    "    train_sensitivity = train_metrics['sensitivity']\n",
    "    train_specificity = train_metrics['specificity']\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_running_acc = 0.0\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "\n",
    "    # Print validation info\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} [Val] - Processing\")\n",
    "\n",
    "    # Iterate through validation data\n",
    "    total_val_batches = len(test_loader)\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, labels) in enumerate(test_loader):\n",
    "            # inputs, labels = inputs.to(device), labels.to(device)\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.view(-1, 1).to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            acc = binary_accuracy(outputs, labels)\n",
    "\n",
    "            # Collect predictions and labels for metrics\n",
    "            preds = (torch.sigmoid(outputs) > 0.5).float().cpu().detach().numpy()\n",
    "            val_preds.extend(preds)\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            val_running_loss += loss.detach().item()\n",
    "            val_running_acc += acc\n",
    "\n",
    "            # Print progress every 10% of batches\n",
    "            if batch_idx % max(1, total_val_batches // 10) == 0 or batch_idx == total_val_batches - 1:\n",
    "                print(f\"  Batch {batch_idx + 1}/{total_val_batches} - Loss: {loss.detach().item():.4f}, Acc: {acc:.4f}\")\n",
    "\n",
    "    val_loss = val_running_loss / len(test_loader)\n",
    "    val_acc = val_running_acc / len(test_loader)\n",
    "\n",
    "    # Calculate sensitivity and specificity for validation data\n",
    "    val_metrics = calculate_metrics(np.array(val_labels), np.array(val_preds))\n",
    "    val_sensitivity = val_metrics['sensitivity']\n",
    "    val_specificity = val_metrics['specificity']\n",
    "\n",
    "    # Update epoch statistics\n",
    "    history.update_epoch(\n",
    "        train_loss, train_acc, val_loss, val_acc,\n",
    "        train_sensitivity, train_specificity,\n",
    "        val_sensitivity, val_specificity\n",
    "    )\n",
    "\n",
    "    # Print epoch summary\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs} - '\n",
    "          f'Train: Loss={train_loss:.4f}, Acc={train_acc:.4f}, Sens={train_sensitivity:.4f}, Spec={train_specificity:.4f} | '\n",
    "          f'Val: Loss={val_loss:.4f}, Acc={val_acc:.4f}, Sens={val_sensitivity:.4f}, Spec={val_specificity:.4f}')\n",
    "\n",
    "    # Save best model (optional)\n",
    "    if epoch > 0 and val_acc > max(history.val_acc['epoch'][:-1]):\n",
    "        torch.save(model.state_dict(), model_save_path.replace('.pt', '_best.pt'))\n",
    "        print(f\"Saved new best model with validation accuracy: {val_acc:.4f}\")"
   ],
   "id": "9fd0ec228395e565",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Starting fold 1/5\n",
      "==================================================\n",
      "Loading data...\n",
      "Training data distribution\n",
      "Label 0: 124154 samples\n",
      "Label 1: 124154 samples\n",
      "Train samples: 248308, Test samples: 75672\n",
      "Model architecture: DeePhage(\n",
      "  (conv): Conv1d(4, 64, kernel_size=(6,), stride=(1,), padding=same)\n",
      "  (relu): ReLU()\n",
      "  (pool): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "Total parameters: 6081\n",
      "Epoch 1/5 [Train] - Processing\n",
      "  Batch 1/7760 - Loss: 0.7388, Acc: 0.4375\n",
      "  Batch 777/7760 - Loss: 0.7163, Acc: 0.6250\n",
      "  Batch 1553/7760 - Loss: 0.5983, Acc: 0.6250\n",
      "  Batch 2329/7760 - Loss: 0.5396, Acc: 0.7188\n",
      "  Batch 3105/7760 - Loss: 0.5215, Acc: 0.7812\n",
      "  Batch 3881/7760 - Loss: 0.4221, Acc: 0.7812\n",
      "  Batch 4657/7760 - Loss: 0.3667, Acc: 0.8750\n",
      "  Batch 5433/7760 - Loss: 0.4861, Acc: 0.7500\n",
      "  Batch 6209/7760 - Loss: 0.3786, Acc: 0.9062\n",
      "  Batch 6985/7760 - Loss: 0.4871, Acc: 0.7500\n",
      "  Batch 7760/7760 - Loss: 0.5559, Acc: 0.6500\n",
      "Epoch 1/5 [Val] - Processing\n",
      "  Batch 1/2365 - Loss: 0.7419, Acc: 0.5938\n",
      "  Batch 237/2365 - Loss: 0.2049, Acc: 0.9688\n",
      "  Batch 473/2365 - Loss: 0.4427, Acc: 0.8438\n",
      "  Batch 709/2365 - Loss: 0.5648, Acc: 0.6875\n",
      "  Batch 945/2365 - Loss: 0.3063, Acc: 0.9688\n",
      "  Batch 1181/2365 - Loss: 0.3370, Acc: 0.9688\n",
      "  Batch 1417/2365 - Loss: 0.3179, Acc: 0.9688\n",
      "  Batch 1653/2365 - Loss: 0.8024, Acc: 0.5000\n",
      "  Batch 1889/2365 - Loss: 0.4094, Acc: 0.8438\n",
      "  Batch 2125/2365 - Loss: 0.7846, Acc: 0.4688\n",
      "  Batch 2361/2365 - Loss: 0.3194, Acc: 0.9375\n",
      "  Batch 2365/2365 - Loss: 0.5005, Acc: 0.7500\n",
      "Epoch 1/5 - Train: Loss=0.5402, Acc=0.7351, Sens=0.7195, Spec=0.7507 | Val: Loss=0.4871, Acc=0.7590, Sens=0.7510, Spec=0.7924\n",
      "Epoch 2/5 [Train] - Processing\n",
      "  Batch 1/7760 - Loss: 0.4753, Acc: 0.7188\n",
      "  Batch 777/7760 - Loss: 0.4154, Acc: 0.7500\n",
      "  Batch 1553/7760 - Loss: 0.4099, Acc: 0.7812\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[20], line 102\u001B[0m\n\u001B[0;32m     99\u001B[0m     loss \u001B[38;5;241m=\u001B[39m criterion(outputs, labels)\n\u001B[0;32m    101\u001B[0m \u001B[38;5;66;03m# Backward pass and optimize\u001B[39;00m\n\u001B[1;32m--> 102\u001B[0m \u001B[43mscaler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscale\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloss\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    103\u001B[0m scaler\u001B[38;5;241m.\u001B[39mstep(optimizer)\n\u001B[0;32m    104\u001B[0m scaler\u001B[38;5;241m.\u001B[39mupdate()\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\dnabert\\lib\\site-packages\\torch\\_tensor.py:648\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    638\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    639\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    640\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    641\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    646\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    647\u001B[0m     )\n\u001B[1;32m--> 648\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    649\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    650\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\dnabert\\lib\\site-packages\\torch\\autograd\\__init__.py:353\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    348\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    350\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    351\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    352\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 353\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    354\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    355\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    356\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    357\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    358\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    359\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    360\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    361\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\dnabert\\lib\\site-packages\\torch\\autograd\\graph.py:824\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[1;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    822\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[0;32m    823\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 824\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Variable\u001B[38;5;241m.\u001B[39m_execution_engine\u001B[38;5;241m.\u001B[39mrun_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    825\u001B[0m         t_outputs, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    826\u001B[0m     )  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    827\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    828\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T09:47:14.463170800Z",
     "start_time": "2025-04-30T09:33:05.049845Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Final evaluation\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "all_train_predictions = []\n",
    "all_train_true_labels = []\n",
    "\n",
    "print(\"Performing final evaluation...\")\n",
    "with torch.no_grad():\n",
    "    # Test predictions\n",
    "    for batch_idx, (inputs, labels) in enumerate(test_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        all_predictions.append(outputs.cpu().numpy())\n",
    "        all_true_labels.append(labels.cpu().numpy())\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Validation batch {batch_idx + 1} - Loss: {loss.detach().item():.4f}, Acc: {acc:.4f}\")\n",
    "\n",
    "    # Train predictions\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        all_train_predictions.append(outputs.cpu().numpy())\n",
    "        all_train_true_labels.append(labels.cpu().numpy())\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Training batch {batch_idx + 1} - Loss: {loss.detach().item():.4f}, Acc: {acc:.4f}\")\n",
    "\n",
    "# Concatenate predictions and true labels\n",
    "predict = np.concatenate(all_predictions).reshape(-1)\n",
    "true_labels = np.concatenate(all_true_labels).reshape(-1)\n",
    "predict_train = np.concatenate(all_train_predictions).reshape(-1)\n",
    "true_train_labels = np.concatenate(all_train_true_labels).reshape(-1)\n",
    "\n",
    "# Save predictions\n",
    "np.savetxt(predict_save_path, predict)\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "# Generate binary predictions\n",
    "predict_binary = (predict > 0.5).astype(int)\n",
    "predict_train_binary = (predict_train > 0.5).astype(int)\n",
    "\n",
    "# Calculate final metrics\n",
    "test_metrics = calculate_metrics(true_labels, predict_binary)\n",
    "train_metrics = calculate_metrics(true_train_labels, predict_train_binary)\n",
    "\n",
    "# Print detailed metrics\n",
    "print(\"\\nFinal Test Metrics:\")\n",
    "print(f\"Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "print(f\"Sensitivity: {test_metrics['sensitivity']:.4f}\")\n",
    "print(f\"Specificity: {test_metrics['specificity']:.4f}\")\n",
    "print(f\"Precision: {test_metrics['precision']:.4f}\")\n",
    "print(f\"F1 Score: {test_metrics['f1_score']:.4f}\")\n",
    "\n",
    "print(\"\\nFinal Train Metrics:\")\n",
    "print(f\"Accuracy: {train_metrics['accuracy']:.4f}\")\n",
    "print(f\"Sensitivity: {train_metrics['sensitivity']:.4f}\")\n",
    "print(f\"Specificity: {train_metrics['specificity']:.4f}\")\n",
    "print(f\"Precision: {train_metrics['precision']:.4f}\")\n",
    "print(f\"F1 Score: {train_metrics['f1_score']:.4f}\")\n",
    "\n",
    "# Generate classification reports\n",
    "report_test = classification_report(true_labels, predict_binary, output_dict=False)\n",
    "print('\\nDetailed Test Classification Report:')\n",
    "print(report_test)\n",
    "report_dic_test = classification_report_csv(report_test, path_save, 0)\n",
    "temp_acc, viru_acc = report_dic_test[0].get('recall'), report_dic_test[1].get('recall')\n",
    "\n",
    "report_train = classification_report(true_train_labels, predict_train_binary, output_dict=False)\n",
    "print('\\nDetailed Train Classification Report:')\n",
    "print(report_train)\n",
    "report_dic_train = classification_report_csv(report_train, path_save, 1)\n",
    "train_temp_acc, train_viru_acc = report_dic_train[0].get('recall'), report_dic_train[1].get('recall')\n",
    "\n",
    "# Create confusion matrix\n",
    "test_cm = confusion_matrix(true_labels, predict_binary)\n",
    "train_cm = confusion_matrix(true_train_labels, predict_train_binary)\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(test_cm)\n",
    "print(\"\\nTrain Confusion Matrix:\")\n",
    "print(train_cm)\n",
    "\n",
    "# Save confusion matrices to CSV\n",
    "pd.DataFrame(test_cm).to_csv(path_save + 'test_confusion_matrix.csv')\n",
    "pd.DataFrame(train_cm).to_csv(path_save + 'train_confusion_matrix.csv')\n",
    "\n",
    "# Save additional metrics to CSV\n",
    "metrics_df = pd.DataFrame({\n",
    "    'dataset': ['test', 'train'],\n",
    "    'accuracy': [test_metrics['accuracy'], train_metrics['accuracy']],\n",
    "    'sensitivity': [test_metrics['sensitivity'], train_metrics['sensitivity']],\n",
    "    'specificity': [test_metrics['specificity'], train_metrics['specificity']],\n",
    "    'precision': [test_metrics['precision'], train_metrics['precision']],\n",
    "    'f1_score': [test_metrics['f1_score'], train_metrics['f1_score']]\n",
    "})\n",
    "metrics_df.to_csv(path_save + 'additional_metrics.csv', index=False)"
   ],
   "id": "a635173efc2f15e1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing final evaluation...\n",
      "Validation batch 1 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 11 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 21 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 31 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 41 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 51 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 61 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 71 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 81 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 91 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 101 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 111 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 121 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 131 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 141 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 151 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 161 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 171 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 181 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 191 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 201 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 211 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 221 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 231 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 241 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 251 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 261 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 271 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 281 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 291 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 301 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 311 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 321 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 331 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 341 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 351 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 361 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 371 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 381 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 391 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 401 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 411 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 421 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 431 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 441 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 451 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 461 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 471 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 481 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 491 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 501 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 511 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 521 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 531 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 541 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 551 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 561 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 571 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 581 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 591 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 601 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 611 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 621 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 631 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 641 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 651 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 661 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 671 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 681 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 691 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 701 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 711 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 721 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 731 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 741 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 751 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 761 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 771 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 781 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 791 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 801 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 811 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 821 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 831 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 841 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 851 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 861 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 871 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 881 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 891 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 901 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 911 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 921 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 931 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 941 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 951 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 961 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 971 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 981 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 991 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1001 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1011 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1021 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1031 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1041 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1051 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1061 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1071 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1081 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1091 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1101 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1111 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1121 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1131 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1141 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1151 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1161 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1171 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1181 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1191 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1201 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1211 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1221 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1231 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1241 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1251 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1261 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1271 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1281 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1291 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1301 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1311 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1321 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1331 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1341 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1351 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1361 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1371 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1381 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1391 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1401 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1411 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1421 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1431 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1441 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1451 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1461 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1471 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1481 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1491 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1501 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1511 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1521 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1531 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1541 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1551 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1561 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1571 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1581 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1591 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1601 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1611 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1621 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1631 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1641 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1651 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1661 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1671 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1681 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1691 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1701 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1711 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1721 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1731 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1741 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1751 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1761 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1771 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1781 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1791 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1801 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1811 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1821 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1831 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1841 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1851 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1861 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1871 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1881 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1891 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1901 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1911 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1921 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1931 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1941 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1951 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1961 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1971 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1981 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 1991 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 2001 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 2011 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 2021 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 2031 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 2041 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 2051 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 2061 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 2071 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 2081 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 2091 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 2101 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 2111 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 2121 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 2131 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 2141 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 2151 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 2161 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 2171 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 2181 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 2191 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 2201 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 2211 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 2221 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 2231 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 2241 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 2251 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 2261 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 2271 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 2281 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 2291 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 2301 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 2311 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 2321 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 2331 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 2341 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 2351 - Loss: 0.4235, Acc: 0.7500\n",
      "Validation batch 2361 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 11 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 21 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 31 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 41 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 51 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 61 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 71 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 81 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 91 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 101 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 111 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 121 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 131 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 141 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 151 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 161 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 171 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 181 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 191 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 201 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 211 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 221 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 231 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 241 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 251 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 261 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 271 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 281 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 291 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 301 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 311 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 321 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 331 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 341 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 351 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 361 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 371 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 381 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 391 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 401 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 411 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 421 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 431 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 441 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 451 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 461 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 471 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 481 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 491 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 501 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 511 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 521 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 531 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 541 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 551 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 561 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 571 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 581 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 591 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 601 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 611 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 621 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 631 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 641 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 651 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 661 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 671 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 681 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 691 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 701 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 711 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 721 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 731 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 741 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 751 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 761 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 771 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 781 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 791 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 801 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 811 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 821 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 831 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 841 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 851 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 861 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 871 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 881 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 891 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 901 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 911 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 921 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 931 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 941 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 951 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 961 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 971 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 981 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 991 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1001 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1011 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1021 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1031 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1041 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1051 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1061 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1071 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1081 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1091 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1101 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1111 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1121 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1131 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1141 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1151 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1161 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1171 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1181 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1191 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1201 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1211 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1221 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1231 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1241 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1251 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1261 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1271 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1281 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1291 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1301 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1311 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1321 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1331 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1341 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1351 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1361 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1371 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1381 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1391 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1401 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1411 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1421 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1431 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1441 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1451 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1461 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1471 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1481 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1491 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1501 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1511 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1521 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1531 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1541 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1551 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1561 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1571 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1581 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1591 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1601 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1611 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1621 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1631 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1641 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1651 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1661 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1671 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1681 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1691 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1701 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1711 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1721 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1731 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1741 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1751 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1761 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1771 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1781 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1791 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1801 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1811 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1821 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1831 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1841 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1851 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1861 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1871 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1881 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1891 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1901 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1911 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1921 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1931 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1941 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1951 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1961 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1971 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1981 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 1991 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2001 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2011 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2021 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2031 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2041 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2051 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2061 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2071 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2081 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2091 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2101 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2111 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2121 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2131 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2141 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2151 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2161 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2171 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2181 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2191 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2201 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2211 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2221 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2231 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2241 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2251 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2261 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2271 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2281 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2291 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2301 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2311 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2321 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2331 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2341 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2351 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2361 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2371 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2381 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2391 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2401 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2411 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2421 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2431 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2441 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2451 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2461 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2471 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2481 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2491 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2501 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2511 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2521 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2531 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2541 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2551 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2561 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2571 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2581 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2591 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2601 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2611 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2621 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2631 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2641 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2651 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2661 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2671 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2681 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2691 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2701 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2711 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2721 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2731 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2741 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2751 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2761 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2771 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2781 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2791 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2801 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2811 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2821 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2831 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2841 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2851 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2861 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2871 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2881 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2891 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2901 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2911 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2921 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2931 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2941 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2951 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2961 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2971 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2981 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 2991 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3001 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3011 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3021 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3031 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3041 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3051 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3061 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3071 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3081 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3091 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3101 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3111 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3121 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3131 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3141 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3151 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3161 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3171 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3181 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3191 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3201 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3211 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3221 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3231 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3241 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3251 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3261 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3271 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3281 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3291 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3301 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3311 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3321 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3331 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3341 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3351 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3361 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3371 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3381 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3391 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3401 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3411 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3421 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3431 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3441 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3451 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3461 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3471 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3481 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3491 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3501 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3511 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3521 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3531 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3541 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3551 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3561 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3571 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3581 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3591 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3601 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3611 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3621 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3631 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3641 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3651 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3661 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3671 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3681 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3691 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3701 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3711 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3721 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3731 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3741 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3751 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3761 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3771 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3781 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3791 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3801 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3811 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3821 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3831 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3841 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3851 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3861 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3871 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3881 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3891 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3901 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3911 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3921 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3931 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3941 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3951 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3961 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3971 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3981 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 3991 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4001 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4011 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4021 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4031 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4041 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4051 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4061 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4071 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4081 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4091 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4101 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4111 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4121 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4131 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4141 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4151 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4161 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4171 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4181 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4191 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4201 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4211 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4221 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4231 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4241 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4251 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4261 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4271 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4281 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4291 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4301 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4311 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4321 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4331 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4341 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4351 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4361 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4371 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4381 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4391 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4401 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4411 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4421 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4431 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4441 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4451 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4461 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4471 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4481 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4491 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4501 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4511 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4521 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4531 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4541 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4551 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4561 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4571 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4581 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4591 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4601 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4611 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4621 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4631 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4641 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4651 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4661 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4671 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4681 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4691 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4701 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4711 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4721 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4731 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4741 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4751 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4761 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4771 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4781 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4791 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4801 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4811 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4821 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4831 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4841 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4851 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4861 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4871 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4881 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4891 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4901 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4911 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4921 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4931 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4941 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4951 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4961 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4971 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4981 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 4991 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5001 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5011 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5021 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5031 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5041 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5051 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5061 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5071 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5081 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5091 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5101 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5111 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5121 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5131 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5141 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5151 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5161 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5171 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5181 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5191 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5201 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5211 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5221 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5231 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5241 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5251 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5261 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5271 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5281 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5291 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5301 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5311 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5321 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5331 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5341 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5351 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5361 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5371 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5381 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5391 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5401 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5411 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5421 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5431 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5441 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5451 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5461 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5471 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5481 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5491 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5501 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5511 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5521 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5531 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5541 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5551 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5561 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5571 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5581 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5591 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5601 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5611 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5621 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5631 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5641 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5651 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5661 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5671 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5681 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5691 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5701 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5711 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5721 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5731 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5741 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5751 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5761 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5771 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5781 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5791 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5801 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5811 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5821 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5831 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5841 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5851 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5861 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5871 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5881 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5891 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5901 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5911 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5921 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5931 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5941 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5951 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5961 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5971 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5981 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 5991 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6001 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6011 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6021 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6031 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6041 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6051 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6061 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6071 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6081 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6091 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6101 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6111 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6121 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6131 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6141 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6151 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6161 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6171 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6181 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6191 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6201 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6211 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6221 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6231 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6241 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6251 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6261 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6271 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6281 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6291 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6301 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6311 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6321 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6331 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6341 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6351 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6361 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6371 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6381 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6391 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6401 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6411 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6421 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6431 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6441 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6451 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6461 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6471 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6481 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6491 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6501 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6511 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6521 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6531 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6541 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6551 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6561 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6571 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6581 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6591 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6601 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6611 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6621 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6631 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6641 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6651 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6661 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6671 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6681 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6691 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6701 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6711 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6721 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6731 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6741 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6751 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6761 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6771 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6781 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6791 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6801 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6811 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6821 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6831 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6841 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6851 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6861 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6871 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6881 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6891 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6901 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6911 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6921 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6931 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6941 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6951 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6961 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6971 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6981 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 6991 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7001 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7011 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7021 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7031 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7041 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7051 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7061 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7071 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7081 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7091 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7101 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7111 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7121 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7131 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7141 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7151 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7161 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7171 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7181 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7191 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7201 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7211 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7221 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7231 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7241 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7251 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7261 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7271 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7281 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7291 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7301 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7311 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7321 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7331 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7341 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7351 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7361 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7371 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7381 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7391 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7401 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7411 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7421 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7431 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7441 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7451 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7461 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7471 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7481 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7491 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7501 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7511 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7521 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7531 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7541 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7551 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7561 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7571 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7581 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7591 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7601 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7611 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7621 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7631 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7641 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7651 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7661 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7671 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7681 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7691 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7701 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7711 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7721 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7731 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7741 - Loss: 0.4235, Acc: 0.7500\n",
      "Training batch 7751 - Loss: 0.4235, Acc: 0.7500\n",
      "Model saved to D:\\implementation\\MyPhageClassificationExperiment\\result\\dee_phage/400_800/fold_1/20250430-162417800_0.0001_32_model.pt\n",
      "\n",
      "Final Test Metrics:\n",
      "Accuracy: 0.7200\n",
      "Sensitivity: 0.6786\n",
      "Specificity: 0.8914\n",
      "Precision: 0.9628\n",
      "F1 Score: 0.7961\n",
      "\n",
      "Final Train Metrics:\n",
      "Accuracy: 0.7955\n",
      "Sensitivity: 0.6885\n",
      "Specificity: 0.9025\n",
      "Precision: 0.8760\n",
      "F1 Score: 0.7710\n",
      "\n",
      "Detailed Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.40      0.89      0.55     14712\n",
      "         1.0       0.96      0.68      0.80     60960\n",
      "\n",
      "    accuracy                           0.72     75672\n",
      "   macro avg       0.68      0.79      0.67     75672\n",
      "weighted avg       0.85      0.72      0.75     75672\n",
      "\n",
      "\n",
      "Detailed Train Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.90      0.82    124154\n",
      "         1.0       0.88      0.69      0.77    124154\n",
      "\n",
      "    accuracy                           0.80    248308\n",
      "   macro avg       0.81      0.80      0.79    248308\n",
      "weighted avg       0.81      0.80      0.79    248308\n",
      "\n",
      "train\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[13115  1597]\n",
      " [19594 41366]]\n",
      "\n",
      "Train Confusion Matrix:\n",
      "[[112052  12102]\n",
      " [ 38674  85480]]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T09:47:14.466171900Z",
     "start_time": "2025-04-30T09:41:27.365599Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Plot and save results\n",
    "history.loss_plot('epoch', val_acc, viru_acc, temp_acc, train_viru_acc, train_temp_acc,\n",
    "                  path_save, max_length, lr_rate, b_size)"
   ],
   "id": "1747e42f40d3431b",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T09:47:14.466171900Z",
     "start_time": "2025-04-30T09:46:17.029385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_loss = history.losses['epoch']\n",
    "val_loss = history.val_loss['epoch']\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.plot(train_loss, label='Train Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "49945932355da717",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T12:05:04.386479Z",
     "start_time": "2025-04-30T12:04:49.696208Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_acc = history.accuracy['epoch']\n",
    "val_acc = history.val_acc['epoch']\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.plot(train_acc, label='Train Loss')\n",
    "plt.plot(val_acc, label='Validation Loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "ecfc149820980337",
   "outputs": [],
   "execution_count": 21
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
